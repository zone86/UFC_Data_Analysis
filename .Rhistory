makeIntegerParam(id = "ntree", lower = 1500L, upper = 2000L),
makeDiscreteParam(id = "bootstrap", default = "by.root", values = c("by.root")),
makeIntegerParam(id = "mtry", lower = 1L, upper = 2L),
makeIntegerParam(id = "nodesize", lower = 1L, upper = 15L),
#makeIntegerLearnerParam(id = "nodedepth", default = -1L),
# makeDiscreteLearnerParam(id = "splitrule", default = "gini",
#                          values = c("gini", "gini.unwt", "gini.hvwt", "random")),
# makeIntegerLearnerParam(id = "nsplit", lower = 0L, default = 0L,
#                         requires = quote(splitrule != "random")), # nsplit is ignored and internally set to 1L for splitrule = "random"
# makeLogicalLearnerParam(id = "split.null", default = FALSE),
# makeDiscreteLearnerParam(id = "importance", default = FALSE, tunable = FALSE,
#                          values = list(`FALSE` = FALSE, `TRUE` = TRUE, "none", "permute", "random", "anti",
#                                        "permute.ensemble", "random.ensemble", "anti.ensemble")),
# makeDiscreteLearnerParam(id = "na.action", default = "na.impute",
#                          values = c("na.omit", "na.impute"), when = "both"),
# FIXME the default in rfsrc() for na.action is na.omit
# makeIntegerLearnerParam(id = "nimpute", default = 1L, lower = 1L),
# makeDiscreteLearnerParam(id = "proximity", default = FALSE, tunable = FALSE,
#                          values = list("inbag", "oob", "all", `TRUE` = TRUE, `FALSE` = FALSE)),
makeIntegerParam(id = "sampsize", lower = 1450L, upper = 2050L,
requires = quote(bootstrap == "by.root")),
makeDiscreteParam(id = "samptype", default = "swr", values = c("swr", "swor"),
requires = quote(bootstrap == "by.root"))
# makeNumericVectorLearnerParam(id = "xvar.wt", lower = 0),
# makeLogicalLearnerParam(id = "forest", default = TRUE, tunable = FALSE),
# makeDiscreteLearnerParam(id = "var.used", default = FALSE, tunable = FALSE,
#                          values = list(`FALSE` = FALSE, "all.trees", "by.tree")),
# makeDiscreteLearnerParam(id = "split.depth", default = FALSE, tunable = FALSE,
#                          values = list(`FALSE` = FALSE, "all.trees", "by.tree")),
# makeIntegerLearnerParam(id = "seed", lower = 0L, tunable = FALSE),
# makeLogicalLearnerParam(id = "do.trace", default = FALSE, tunable = FALSE, when = "both"), # is currently ignored
# makeLogicalLearnerParam(id = "membership", default = TRUE, tunable = FALSE),
# makeLogicalLearnerParam(id = "statistics", default = FALSE, tunable = FALSE),
# makeLogicalLearnerParam(id = "tree.err", default = FALSE, tunable = FALSE)
)
RF.param = c()
RF.result = data.frame(matrix(nrow=30,ncol=1))
for (i in 1:20) {
set.seed(i)
RF.res = tuneParams(makeLearner("classif.randomForestSRC", predict.type = "prob"),
task, cv3, par.set = RF.ps, control = tune.ctrl.grid,
measures = mlr::brier,
show.info = TRUE)
RF.param[[i]] = RF.res$x
RF.result[i,] = RF.res$y
}
parallelStop()
gc()
RF.param
RF.result
parallelStartSocket(cpus = 2, level = "mlr.tuneParams")
RF.result = data.frame(matrix(nrow=3,ncol=1))
for (i in 1:3) {
set.seed(i)
RF.res = tuneParams(makeLearner("classif.randomForestSRC", predict.type = "prob"),
task, cv3, par.set = RF.ps, control = tune.ctrl.grid,
measures = mlr::brier,
show.info = TRUE)
RF.param[[i]] = RF.res$x
RF.result[i,] = RF.res$y
}
RF.param
parallelStop()
gc()
RF.param[[1]]
RF.Mod = setHyperPars(RF.Mod, par.vals = RF.param[[1]] )
RF.Mod
######################################################################################
# models that sample randomly and can handle missing values
RF.rand = data.frame(matrix(nrow = 0, ncol = 3))
parallelStartSocket(cpus = 3, level = "mlr.resample")
for (i in 1:20) {
set.seed(i)
RF.CV = resample(RF.Mod, task, cv3, measures = list(brier,auc,acc), extract = function(x) getLearnerModel(x))
RF.rand[i,] = RF.CV$aggr
}
parallelStop()
gc()
sapply(RF.rand, function (x) mean(x))
RF.Mod
generateLearningCurveData(learners=RF.Mod,task=task)
RF.learn = generateLearningCurveData(learners=RF.Mod,task=task)
plotLearningCurve(RF.learn)
RF.learn = generateLearningCurveData(learners=RF.Mod,task=task, measures = list(brier,auc,acc))
plotLearningCurve(RF.learn)
plotLearningCurve(RF.learn) + ggtitle('RF Learning Curve')
plotLearningCurve(RF.learn) + ggtitle('RF Learning Curve after feature selection and optimization')
plotLearningCurve(RF.learn) + ggtitle('Random Forest learning curve after feature selection and optimization')
##########################################################################################
# create models after feature selection
Df = allDatCleaned %>%
#select(bwrd.feats$x, Winner)
#select(head(all_feats$name[reorder(all_feats$name,-all_feats$Mean)],16), Winner)
select(B_Age, R_Age, R_Younger, AgeDifference, R_Heavier)
# remove no contests and draws as they are extremely rare
Df = Df[!(Df$Winner %in% c('no contest','draw')),]
# encode logicals as factors
Df$Winner = factor(Df$Winner, levels = c('blue','red'))
Df$R_Younger = factor(Df$R_Younger, levels = c('FALSE','TRUE'))
Df$R_Heavier = factor(Df$R_Heavier, levels = c('FALSE','TRUE'))
# find all NaN's and replace with NA
idx = sapply(Df, function(x) is.nan(x))
Df[idx] <- NA
# use Df1 if you choose to omit NAs...
Df1 = na.omit(Df)
##########################################################################################
# create models after feature selection
Df = allDatCleaned %>%
#select(bwrd.feats$x, Winner)
#select(head(all_feats$name[reorder(all_feats$name,-all_feats$Mean)],16), Winner)
select(B_Age, R_Age, R_Younger, AgeDifference, R_Heavier,
# R_Avg_Total_Strikes_Head.Total.Strikes_Percentage,
# R_Avg_Total_Strikes_Head.Significant.Strikes_Percentage,
# R_Avg_Total_Strikes_Total.Strikes_Percentage,
# R_Avg_Total_Strikes_Significant.Strikes_Percentage,
# R_Avg_Total_Strikes_Distance.Body.Punches_Percentage,
# B_Avg_Total_Strikes_Significant.Strikes_Percentage,
# B_Avg_Total_Strikes_Head.Total.Strikes_Percentage,
# B_Avg_Total_Strikes_Head.Significant.Strikes_Percentage,
# B_Avg_Total_Strikes_Total.Strikes_Percentage,
# B_Avg_Total_Strikes_Distance.Leg.Kicks_Percentage,
# B_Avg_Total_Strikes_Distance.Body.Kicks_Percentage,
# B_Avg_Total_Strikes_Kicks_Percentage,
# B_Avg_Total_Strikes_Punches_Percentage,
# B_Avg_Total_Strikes_Clinch.Significant.Kicks_Percentage,
# B_Avg_Total_Strikes_Distance.Head.Punches_Percentage,
# B_Avg_Total_Strikes_Distance.Head.Kicks_Percentage,
# B_Avg_Total_Strikes_Clinch.Significant.Kicks_Percentage,
# B_Avg_Total_Strikes_Ground.Significant.Punches_Percentage,
Winner)
# remove no contests and draws as they are extremely rare
Df = Df[!(Df$Winner %in% c('no contest','draw')),]
# encode logicals as factors
Df$Winner = factor(Df$Winner, levels = c('blue','red'))
Df$R_Younger = factor(Df$R_Younger, levels = c('FALSE','TRUE'))
Df$R_Heavier = factor(Df$R_Heavier, levels = c('FALSE','TRUE'))
# find all NaN's and replace with NA
idx = sapply(Df, function(x) is.nan(x))
Df[idx] <- NA
# use Df1 if you choose to omit NAs...
Df1 = na.omit(Df)
# use Df1 if you choose to omit NAs...
Df1 = na.omit(Df)
task = makeClassifTask(target = 'Winner', data = Df1)
# if you want to standardize the data, otherwise skip
task = normalizeFeatures(obj = task, method = 'standardize',
cols = colnames(Df1)[- which(sapply(Df1,is.factor))] )
log.Mod = makeLearner('classif.logreg', predict.type = "prob")
cvglmnet.Mod = makeLearner('classif.cvglmnet', predict.type = "prob", par.vals = list(alpha = 0)) # Ridge model
svm.Mod = makeLearner('classif.svm', predict.type = "prob")
nnet.Mod = makeLearner('classif.nnet', predict.type = "prob")
###########################################################################
# find the accuracy of models with 100 different random seeds
# models that cannot handle missing values
logMod.rand = data.frame(matrix(nrow = 0, ncol = 3))
for (i in 1:20) {
set.seed(i)
logMod.CV = resample(log.Mod, task, cv3, measures = list(brier,auc,acc), extract = function(x) getLearnerModel(x)$coefficients)
logMod.rand[i,] = logMod.CV$aggr
}
mean(logMod.rand$X1) # Brier score
mean(logMod.rand$X2) # AUC
mean(logMod.rand$X3) # Accuracy
log.learn = generateLearningCurveData(learners=log.Mod,task=task, measures = list(brier,auc,acc))
log.learn = generateLearningCurveData(learners=log.Mod,task=task, measures = list(brier,auc,acc))
plotLearningCurve(log.learn) + ggtitle('Logistic Regression learning curve with "simple" features')
##############################################################################################
# declare hyperparam search space for each of the models
# that cant handle missing values...
glmnet.ps = makeParamSet( makeNumericParam(id = "alpha", lower = 0, upper = 1),
makeNumericParam(id = "s", lower = 0, upper = 1, when = "predict"),
#makeLogicalLearnerParam(id = "exact", default = FALSE, when = "predict"),
makeIntegerParam(id = "nlambda", lower = 100L, upper = 200L)
# makeNumericLearnerParam(id = "lambda.min.ratio", lower = 0, upper = 1),
# #makeNumericVectorParam(id = "lambda", lower = 0, ),
# makeLogicalLearnerParam(id = "standardize", default = TRUE),
# makeLogicalLearnerParam(id = "intercept", default = TRUE),
# makeNumericLearnerParam(id = "thresh", default = 1e-07, lower = 0),
# makeIntegerLearnerParam(id = "dfmax", lower = 0L),
# makeIntegerLearnerParam(id = "pmax", lower = 0L),
# makeIntegerVectorLearnerParam(id = "exclude", lower = 1L),
# makeNumericVectorLearnerParam(id = "penalty.factor", lower = 0, upper = 1),
# makeNumericVectorLearnerParam(id = "lower.limits", upper = 0),
# makeNumericVectorLearnerParam(id = "upper.limits", lower = 0),
# makeIntegerLearnerParam(id = "maxit", default = 100000L, lower = 1L),
# makeDiscreteLearnerParam(id = "type.logistic", values = c("Newton", "modified.Newton")),
# makeDiscreteLearnerParam(id = "type.multinomial", values = c("ungrouped", "grouped")),
# makeNumericLearnerParam(id = "fdev", default = 1.0e-5, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "devmax", default = 0.999, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "eps", default = 1.0e-6, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "big", default = 9.9e35),
# makeIntegerLearnerParam(id = "mnlam", default = 5, lower = 1),
# makeNumericLearnerParam(id = "pmin", default = 1.0e-9, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "exmx", default = 250.0),
# makeNumericLearnerParam(id = "prec", default = 1e-10),
# makeIntegerLearnerParam(id = "mxit", default = 100L, lower = 1L),
# makeUntypedLearnerParam(id = "offset", default = NULL),
# makeDiscreteLearnerParam(id = "type.gaussian", values = c("covariance", "naive"), requires = quote(family == "gaussian")),
# makeLogicalLearnerParam(id = "relax", default = FALSE)
)
##############################################################################################
# declare hyperparam search space for each of the models
# that cant handle missing values...
glmnet.ps = makeParamSet( makeNumericParam(id = "alpha", lower = 0, upper = 1),
makeNumericParam(id = "s", lower = 0, upper = 1),
#makeLogicalLearnerParam(id = "exact", default = FALSE, when = "predict"),
makeIntegerParam(id = "nlambda", lower = 100L, upper = 200L)
# makeNumericLearnerParam(id = "lambda.min.ratio", lower = 0, upper = 1),
# #makeNumericVectorParam(id = "lambda", lower = 0, ),
# makeLogicalLearnerParam(id = "standardize", default = TRUE),
# makeLogicalLearnerParam(id = "intercept", default = TRUE),
# makeNumericLearnerParam(id = "thresh", default = 1e-07, lower = 0),
# makeIntegerLearnerParam(id = "dfmax", lower = 0L),
# makeIntegerLearnerParam(id = "pmax", lower = 0L),
# makeIntegerVectorLearnerParam(id = "exclude", lower = 1L),
# makeNumericVectorLearnerParam(id = "penalty.factor", lower = 0, upper = 1),
# makeNumericVectorLearnerParam(id = "lower.limits", upper = 0),
# makeNumericVectorLearnerParam(id = "upper.limits", lower = 0),
# makeIntegerLearnerParam(id = "maxit", default = 100000L, lower = 1L),
# makeDiscreteLearnerParam(id = "type.logistic", values = c("Newton", "modified.Newton")),
# makeDiscreteLearnerParam(id = "type.multinomial", values = c("ungrouped", "grouped")),
# makeNumericLearnerParam(id = "fdev", default = 1.0e-5, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "devmax", default = 0.999, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "eps", default = 1.0e-6, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "big", default = 9.9e35),
# makeIntegerLearnerParam(id = "mnlam", default = 5, lower = 1),
# makeNumericLearnerParam(id = "pmin", default = 1.0e-9, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "exmx", default = 250.0),
# makeNumericLearnerParam(id = "prec", default = 1e-10),
# makeIntegerLearnerParam(id = "mxit", default = 100L, lower = 1L),
# makeUntypedLearnerParam(id = "offset", default = NULL),
# makeDiscreteLearnerParam(id = "type.gaussian", values = c("covariance", "naive"), requires = quote(family == "gaussian")),
# makeLogicalLearnerParam(id = "relax", default = FALSE)
)
glmnet.param = c()
#############################################################################################
# model optimization and hyperparameter tuning
# control settings for MBO
ctrl = makeMBOControl()
ctrl = setMBOControlTermination(ctrl, iters = 30)
ctrl = setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI(),
opt = "focussearch", opt.focussearch.points = 300L)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl)
tune.ctrl.grid = makeTuneControlGrid(resolution = 3L)
glmnet.param = c()
glmnet.result = data.frame(matrix(nrow=30,ncol=1))
for (i in 1:30) {
set.seed(i)
glmnet.res = tuneParams(makeLearner("classif.glmnet", predict.type = "prob"),
task, cv3, par.set = glmnet.ps, control = tune.ctrl,
measures = mlr::brier,
show.info = TRUE)
glmnet.param[[i]] = glmnet.res$x
glmnet.result[i,] = glmnet.res$y
}
which.min(glmnet.result$matrix.nrow...10..ncol...1.)
which.min(glmnet.result$matrix.nrow...30..ncol...1.)
glmnet.param[[2]]
cvglmnet.Mod = setHyperPars(cvglmnet.Mod, par.vals = glmnet.param[[2]] )
glmnet.CV
cvglmnet.Mod$par.set
##############################################################################################
# declare hyperparam search space for each of the models
# that cant handle missing values...
glmnet.ps = makeParamSet( makeNumericParam(id = "alpha", lower = 0, upper = 1),
#makeNumericParam(id = "s", lower = 0, upper = 1),
#makeLogicalLearnerParam(id = "exact", default = FALSE, when = "predict"),
makeIntegerParam(id = "nlambda", lower = 100L, upper = 200L)
# makeNumericLearnerParam(id = "lambda.min.ratio", lower = 0, upper = 1),
# #makeNumericVectorParam(id = "lambda", lower = 0, ),
# makeLogicalLearnerParam(id = "standardize", default = TRUE),
# makeLogicalLearnerParam(id = "intercept", default = TRUE),
# makeNumericLearnerParam(id = "thresh", default = 1e-07, lower = 0),
# makeIntegerLearnerParam(id = "dfmax", lower = 0L),
# makeIntegerLearnerParam(id = "pmax", lower = 0L),
# makeIntegerVectorLearnerParam(id = "exclude", lower = 1L),
# makeNumericVectorLearnerParam(id = "penalty.factor", lower = 0, upper = 1),
# makeNumericVectorLearnerParam(id = "lower.limits", upper = 0),
# makeNumericVectorLearnerParam(id = "upper.limits", lower = 0),
# makeIntegerLearnerParam(id = "maxit", default = 100000L, lower = 1L),
# makeDiscreteLearnerParam(id = "type.logistic", values = c("Newton", "modified.Newton")),
# makeDiscreteLearnerParam(id = "type.multinomial", values = c("ungrouped", "grouped")),
# makeNumericLearnerParam(id = "fdev", default = 1.0e-5, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "devmax", default = 0.999, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "eps", default = 1.0e-6, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "big", default = 9.9e35),
# makeIntegerLearnerParam(id = "mnlam", default = 5, lower = 1),
# makeNumericLearnerParam(id = "pmin", default = 1.0e-9, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "exmx", default = 250.0),
# makeNumericLearnerParam(id = "prec", default = 1e-10),
# makeIntegerLearnerParam(id = "mxit", default = 100L, lower = 1L),
# makeUntypedLearnerParam(id = "offset", default = NULL),
# makeDiscreteLearnerParam(id = "type.gaussian", values = c("covariance", "naive"), requires = quote(family == "gaussian")),
# makeLogicalLearnerParam(id = "relax", default = FALSE)
)
glmnet.param = c()
glmnet.result = data.frame(matrix(nrow=30,ncol=1))
parallelStartSocket(cpus = 3, level = "mlr.tuneParams")
for (i in 1:30) {
set.seed(i)
glmnet.res = tuneParams(makeLearner("classif.glmnet", predict.type = "prob"),
task, cv3, par.set = glmnet.ps, control = tune.ctrl,
measures = mlr::brier,
show.info = TRUE)
glmnet.param[[i]] = glmnet.res$x
glmnet.result[i,] = glmnet.res$y
}
which.min(glmnet.result$matrix.nrow...30..ncol...1.)
cvglmnet.Mod = setHyperPars(cvglmnet.Mod, par.vals = glmnet.param[[2]] )
cvglmnet.Mod
parallelStop()
gc()
##############################################################################################
# declare hyperparam search space for each of the models
# that cant handle missing values...
glmnet.ps = makeParamSet( makeNumericParam(id = "alpha", lower = 0, upper = 1),
#makeNumericParam(id = "s", lower = 0, upper = 1),
#makeLogicalLearnerParam(id = "exact", default = FALSE, when = "predict"),
makeIntegerParam(id = "nlambda", lower = 100L, upper = 200L),
# makeNumericLearnerParam(id = "lambda.min.ratio", lower = 0, upper = 1),
# #makeNumericVectorParam(id = "lambda", lower = 0, ),
# makeLogicalLearnerParam(id = "standardize", default = TRUE),
# makeLogicalLearnerParam(id = "intercept", default = TRUE),
# makeNumericLearnerParam(id = "thresh", default = 1e-07, lower = 0),
# makeIntegerLearnerParam(id = "dfmax", lower = 0L),
# makeIntegerLearnerParam(id = "pmax", lower = 0L),
# makeIntegerVectorLearnerParam(id = "exclude", lower = 1L),
# makeNumericVectorLearnerParam(id = "penalty.factor", lower = 0, upper = 1),
# makeNumericVectorLearnerParam(id = "lower.limits", upper = 0),
# makeNumericVectorLearnerParam(id = "upper.limits", lower = 0),
# makeIntegerLearnerParam(id = "maxit", default = 100000L, lower = 1L),
makeDiscreteParam(id = "type.logistic", values = c("Newton", "modified.Newton"))
# makeDiscreteLearnerParam(id = "type.multinomial", values = c("ungrouped", "grouped")),
# makeNumericLearnerParam(id = "fdev", default = 1.0e-5, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "devmax", default = 0.999, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "eps", default = 1.0e-6, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "big", default = 9.9e35),
# makeIntegerLearnerParam(id = "mnlam", default = 5, lower = 1),
# makeNumericLearnerParam(id = "pmin", default = 1.0e-9, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "exmx", default = 250.0),
# makeNumericLearnerParam(id = "prec", default = 1e-10),
# makeIntegerLearnerParam(id = "mxit", default = 100L, lower = 1L),
# makeUntypedLearnerParam(id = "offset", default = NULL),
# makeDiscreteLearnerParam(id = "type.gaussian", values = c("covariance", "naive"), requires = quote(family == "gaussian")),
# makeLogicalLearnerParam(id = "relax", default = FALSE)
)
glmnet.param = c()
glmnet.result = data.frame(matrix(nrow=30,ncol=1))
parallelStartSocket(cpus = 3, level = "mlr.tuneParams")
for (i in 1:30) {
set.seed(i)
glmnet.res = tuneParams(makeLearner("classif.glmnet", predict.type = "prob"),
task, cv3, par.set = glmnet.ps, control = tune.ctrl,
measures = mlr::brier,
show.info = TRUE)
glmnet.param[[i]] = glmnet.res$x
glmnet.result[i,] = glmnet.res$y
}
which.min(glmnet.result$matrix.nrow...30..ncol...1.)
glmnet.param[[2]]
cvglmnet.Mod = setHyperPars(cvglmnet.Mod, par.vals = glmnet.param[[2]] )
cvglmnet.Mod
parallelStop()
gc()
glmnet.rand = data.frame(matrix(nrow = 0, ncol = 3))
for (i in 1:20) {
set.seed(i)
glmnet.CV = resample(cvglmnet.Mod, task, cv3, measures = list(brier,auc,acc), extract = function(x) getLearnerModel(x))
glmnet.rand[i,] = glmnet.CV$aggr
}
mean(glmnet.rand$X1)
mean(glmnet.rand$X2)
mean(glmnet.rand$X3)
cvglmnet.Mod
tune.ctrl.grid = makeTuneControlGrid(resolution = 3L)
##############################################################################################
# declare hyperparam search space for each of the models
# that cant handle missing values...
glmnet.ps = makeParamSet( makeNumericParam(id = "alpha", lower = 0, upper = .01),
#makeNumericParam(id = "s", lower = 0, upper = 1),
#makeLogicalLearnerParam(id = "exact", default = FALSE, when = "predict"),
makeIntegerParam(id = "nlambda", lower = 100L, upper = 200L),
# makeNumericLearnerParam(id = "lambda.min.ratio", lower = 0, upper = 1),
# #makeNumericVectorParam(id = "lambda", lower = 0, ),
# makeLogicalLearnerParam(id = "standardize", default = TRUE),
# makeLogicalLearnerParam(id = "intercept", default = TRUE),
# makeNumericLearnerParam(id = "thresh", default = 1e-07, lower = 0),
# makeIntegerLearnerParam(id = "dfmax", lower = 0L),
# makeIntegerLearnerParam(id = "pmax", lower = 0L),
# makeIntegerVectorLearnerParam(id = "exclude", lower = 1L),
# makeNumericVectorLearnerParam(id = "penalty.factor", lower = 0, upper = 1),
# makeNumericVectorLearnerParam(id = "lower.limits", upper = 0),
# makeNumericVectorLearnerParam(id = "upper.limits", lower = 0),
# makeIntegerLearnerParam(id = "maxit", default = 100000L, lower = 1L),
makeDiscreteParam(id = "type.logistic", values = c("Newton", "modified.Newton"))
# makeDiscreteLearnerParam(id = "type.multinomial", values = c("ungrouped", "grouped")),
# makeNumericLearnerParam(id = "fdev", default = 1.0e-5, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "devmax", default = 0.999, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "eps", default = 1.0e-6, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "big", default = 9.9e35),
# makeIntegerLearnerParam(id = "mnlam", default = 5, lower = 1),
# makeNumericLearnerParam(id = "pmin", default = 1.0e-9, lower = 0, upper = 1),
# makeNumericLearnerParam(id = "exmx", default = 250.0),
# makeNumericLearnerParam(id = "prec", default = 1e-10),
# makeIntegerLearnerParam(id = "mxit", default = 100L, lower = 1L),
# makeUntypedLearnerParam(id = "offset", default = NULL),
# makeDiscreteLearnerParam(id = "type.gaussian", values = c("covariance", "naive"), requires = quote(family == "gaussian")),
# makeLogicalLearnerParam(id = "relax", default = FALSE)
)
glmnet.param = c()
glmnet.result = data.frame(matrix(nrow=30,ncol=1))
parallelStartSocket(cpus = 2, level = "mlr.tuneParams")
for (i in 1:3) {
set.seed(i)
glmnet.res = tuneParams(makeLearner("classif.glmnet", predict.type = "prob"),
task, cv3, par.set = glmnet.ps, control = tune.ctrl.grid,
measures = mlr::brier,
show.info = TRUE)
glmnet.param[[i]] = glmnet.res$x
glmnet.result[i,] = glmnet.res$y
}
which.min(glmnet.result$matrix.nrow...30..ncol...1.)
glmnet.param[[2]]
parallelStop()
gc()
glmnet.result = data.frame(matrix(nrow=10,ncol=1))
glmnet.param = c()
parallelStartSocket(cpus = 2, level = "mlr.tuneParams")
for (i in 1:10) {
set.seed(i)
glmnet.res = tuneParams(makeLearner("classif.glmnet", predict.type = "prob"),
task, cv3, par.set = glmnet.ps, control = tune.ctrl.grid,
measures = mlr::brier,
show.info = TRUE)
glmnet.param[[i]] = glmnet.res$x
glmnet.result[i,] = glmnet.res$y
}
parallelStop()
gc()
which.min(glmnet.result$matrix.nrow...30..ncol...1.)
which.min(glmnet.result$matrix.nrow...10..ncol...1.)
glmnet.param[[2]]
cvglmnet.Mod = setHyperPars(cvglmnet.Mod, par.vals = glmnet.param[[2]] )
cvglmnet.Mod
glmnet.rand = data.frame(matrix(nrow = 0, ncol = 3))
for (i in 1:20) {
set.seed(i)
glmnet.CV = resample(cvglmnet.Mod, task, cv3, measures = list(brier,auc,acc), extract = function(x) getLearnerModel(x))
glmnet.rand[i,] = glmnet.CV$aggr
}
mean(glmnet.rand$X1)
mean(glmnet.rand$X2)
mean(glmnet.rand$X3)
glmnet.learn = generateLearningCurveData(learners=cvglmnet.Mod,task=task, measures = list(brier,auc,acc))
plotLearningCurve(glmnet.learn) + ggtitle('Ridge Regression learning curve with "simple" features')
#############################################################################################
# model optimization and hyperparameter tuning
# control settings for MBO
ctrl = makeMBOControl()
ctrl = setMBOControlTermination(ctrl, iters = 30)
ctrl = setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI(),
opt = "focussearch", opt.focussearch.points = 300L)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl)
nnet.ps = makeParamSet(
#makeIntegerLearnerParam(id = "size", default = 3L, lower = 0L),
# FIXME size seems to have no default in nnet(). If it has, par.vals is redundant
makeIntegerParam(id = "maxit", lower = 10L, upper = 50L),
# nnet seems to set these manually and hard for classification.....
#     makeLogicalLearnerParam(id = "linout", default = FALSE, requires = quote(entropy == FALSE && softmax == FALSE && censored == FALSE)),
#     makeLogicalLearnerParam(id = "entropy", default = FALSE, requires = quote(linout == FALSE && softmax == FALSE && censored == FALSE)),
#     makeLogicalLearnerParam(id = "softmax", default = FALSE, requires = quote(entropy == FALSE && linout == FALSE && censored == FALSE)),
#     makeLogicalLearnerParam(id = "censored", default = FALSE, requires = quote(linout == FALSE && softmax == FALSE && entropy == FALSE)),
#makeLogicalLearnerParam(id = "skip", default = FALSE),
makeNumericParam(id = "rang", lower = 0.1, upper = 0.9),
makeNumericParam(id = "decay", lower = 0, upper = 1),
makeIntegerParam(id = "MaxNWts", lower = 100L, upper = 2000L, tunable = TRUE)
)
# tune models using 100 different seeds
nnet.result = data.frame(matrix(nrow=100,ncol=1))
nnet.param = c()
# tune models using different seeds
nnet.result = data.frame(matrix(nrow=100,ncol=1))
nnet.param = c()
parallelStartSocket(cpus = 2, level = "mlr.tuneParams")
for (i in 1:30) {
set.seed(i)
nnet.res = tuneParams(makeLearner("classif.nnet", predict.type = "prob"),
task, cv3, par.set = nnet.ps, control = tune.ctrl,
measures = mlr::brier,
show.info = TRUE)
nnet.param[[i]] = nnet.res$x
nnet.result[i,] = nnet.res$y
}
parallelStop()
gc()
nnet.idx = which.min(nnet.result$matrix.nrow...100..ncol...1.)
nnet.param[nnet.idx]
nnet.Mod = setHyperPars(nnet.Mod, par.vals = nnet.param[nnet.idx])
nnet.param[nnet.idx]
nnet.param[[nnet.idx]]
nnet.Mod = setHyperPars(nnet.Mod, par.vals = nnet.param[[nnet.idx]])
nnet.Mod
nnet.rand = data.frame(matrix(nrow = 0, ncol = 3))
for (i in 1:20) {
set.seed(i)
nnet.CV = resample(nnet.Mod, task, cv3, measures = list(brier,auc,acc), extract = function(x) getLearnerModel(x))
nnet.rand[i,] = nnet.CV$aggr
}
sapply(nnet.rand, function (x) mean(x))
nnet.learn = generateLearningCurveData(learners=nnet.Mod,task=task, measures = list(brier,auc,acc))
plotLearningCurve(nnet.learn) + ggtitle('Neural Net learning curve with "simple" features')
nnet.Mod
